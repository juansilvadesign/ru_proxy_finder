import aiohttp
import asyncio
import json
import os
from datetime import datetime
import re
import sys
from rich.console import Console
from rich.table import Table
from bs4 import BeautifulSoup
import random
import requests

console = Console()

# –ö–æ–Ω—Ñ–∏–≥—É—Ä–∞—Ü–∏—è
BASE_DIR = os.path.dirname(os.path.abspath(__file__))
DATA_DIR = os.path.join(BASE_DIR, "data")
os.makedirs(DATA_DIR, exist_ok=True)

class RussianProxyFinder:
    def __init__(self):
        self.proxies = []
        self.session = None
        self.russian_proxies = []
    
    async def initialize(self):
        self.session = aiohttp.ClientSession()
    
    async def close(self):
        if self.session:
            await self.session.close()
    
    async def get_proxies_from_api(self):
        # –°–æ–±–∏—Ä–∞–µ–º –ø—Ä–æ–∫—Å–∏ –∏–∑ —Ä–∞–∑–Ω—ã—Ö API –∏—Å—Ç–æ—á–Ω–∏–∫–æ–≤
        tasks = [
            self.get_proxies_from_proxylist_download(),
            self.get_proxies_from_freeproxy_world(),
            self.get_proxies_from_proxy_list_ru(),
            self.get_proxies_from_hidemy_name(),
            self.get_proxies_from_geonode(),
            self.get_proxies_from_free_proxy_list(),
            self.get_proxies_from_proxy_list_download(),
            self.get_proxies_from_proxy_list_org(),
            # –°–ø–µ—Ü–∏–∞–ª–∏–∑–∏—Ä–æ–≤–∞–Ω–Ω—ã–µ —Ä–æ—Å—Å–∏–π—Å–∫–∏–µ –∏—Å—Ç–æ—á–Ω–∏–∫–∏
            self.get_proxies_from_proxyscrape_ru(),
            self.get_proxies_from_proxyservers_ru(),
            self.get_proxies_from_2ip_ru(),
            self.get_proxies_from_proxy24_net_ru(),
            # –ù–æ–≤—ã–µ –∏—Å—Ç–æ—á–Ω–∏–∫–∏ –ø—Ä–æ–∫—Å–∏
            self.get_proxies_from_htmlweb_api(),
            self.get_proxies_from_proxy5_net(),
            self.get_proxies_from_fineproxy_org(),
            self.get_proxies_from_proxyfreeonly(),
            self.get_proxies_from_good_proxies_ru(),
            self.get_proxies_from_iproyal_ru(),
        ]
        
        await asyncio.gather(*tasks)
        console.print(f"[bold green]–ù–∞–π–¥–µ–Ω–æ {len(self.proxies)} –ø—Ä–æ–∫—Å–∏ –∏–∑ API –∏—Å—Ç–æ—á–Ω–∏–∫–æ–≤")

    async def get_proxies_from_proxylist_download(self):
        try:
            url = "https://www.proxy-list.download/api/v1/get?type=http"
            async with self.session.get(url, timeout=10) as response:
                if response.status == 200:
                    text = await response.text()
                    proxy_list = text.strip().split('\r\n')
                    self.proxies.extend(proxy_list)
                    console.print(f"–ü–æ–ª—É—á–µ–Ω–æ {len(proxy_list)} –ø—Ä–æ–∫—Å–∏ –æ—Ç proxy-list.download")
        except Exception as e:
            console.print(f"[red]–û—à–∏–±–∫–∞ –ø—Ä–∏ –ø–æ–ª—É—á–µ–Ω–∏–∏ –ø—Ä–æ–∫—Å–∏ –æ—Ç proxy-list.download: {e}")
    
    async def get_proxies_from_freeproxy_world(self):
        try:
            url = "https://www.freeproxy.world/"
            async with self.session.get(url, timeout=10) as response:
                if response.status == 200:
                    text = await response.text()
                    soup = BeautifulSoup(text, 'html.parser')
                    rows = soup.select('.table-striped tbody tr')
                    proxy_list = []
                    for row in rows:
                        columns = row.select('td')
                        if len(columns) >= 2:
                            ip = columns[0].text.strip()
                            port = columns[1].text.strip()
                            country = columns[6].text.strip() if len(columns) > 6 else ""
                            if country and "Russia" in country or "RU" in country:
                                proxy_list.append(f"{ip}:{port}")
                    self.proxies.extend(proxy_list)
                    console.print(f"–ü–æ–ª—É—á–µ–Ω–æ {len(proxy_list)} –ø—Ä–æ–∫—Å–∏ –æ—Ç freeproxy.world")
        except Exception as e:
            console.print(f"[red]–û—à–∏–±–∫–∞ –ø—Ä–∏ –ø–æ–ª—É—á–µ–Ω–∏–∏ –ø—Ä–æ–∫—Å–∏ –æ—Ç freeproxy.world: {e}")

    async def get_proxies_from_proxy_list_ru(self):
        try:
            url = "https://proxy-list.ru/russian-proxy-list"
            async with self.session.get(url, timeout=10) as response:
                if response.status == 200:
                    text = await response.text()
                    soup = BeautifulSoup(text, 'html.parser')
                    rows = soup.select('.proxy-list-table tr')
                    proxy_list = []
                    for row in rows[1:]:  # –ü—Ä–æ–ø—É—Å–∫–∞–µ–º –∑–∞–≥–æ–ª–æ–≤–æ–∫
                        columns = row.select('td')
                        if len(columns) >= 2:
                            ip = columns[0].text.strip()
                            port = columns[1].text.strip()
                            proxy_list.append(f"{ip}:{port}")
                    self.proxies.extend(proxy_list)
                    console.print(f"–ü–æ–ª—É—á–µ–Ω–æ {len(proxy_list)} –ø—Ä–æ–∫—Å–∏ –æ—Ç proxy-list.ru")
        except Exception as e:
            console.print(f"[red]–û—à–∏–±–∫–∞ –ø—Ä–∏ –ø–æ–ª—É—á–µ–Ω–∏–∏ –ø—Ä–æ–∫—Å–∏ –æ—Ç proxy-list.ru: {e}")
    
    async def get_proxies_from_hidemy_name(self):
        try:
            url = "https://hidemy.name/ru/proxy-list/?country=RU"
            headers = {
                'User-Agent': 'Mozilla/5.0 (Windows NT 10.0; Win64; x64) AppleWebKit/537.36 (KHTML, like Gecko) Chrome/58.0.3029.110 Safari/537.3'
            }
            async with self.session.get(url, headers=headers, timeout=10) as response:
                if response.status == 200:
                    text = await response.text()
                    soup = BeautifulSoup(text, 'html.parser')
                    rows = soup.select('.table_block tbody tr')
                    proxy_list = []
                    for row in rows:
                        columns = row.select('td')
                        if len(columns) >= 2:
                            ip = columns[0].text.strip()
                            port = columns[1].text.strip()
                            proxy_list.append(f"{ip}:{port}")
                    self.proxies.extend(proxy_list)
                    console.print(f"–ü–æ–ª—É—á–µ–Ω–æ {len(proxy_list)} –ø—Ä–æ–∫—Å–∏ –æ—Ç hidemy.name")
        except Exception as e:
            console.print(f"[red]–û—à–∏–±–∫–∞ –ø—Ä–∏ –ø–æ–ª—É—á–µ–Ω–∏–∏ –ø—Ä–æ–∫—Å–∏ –æ—Ç hidemy.name: {e}")
    
    async def get_proxies_from_geonode(self):
        try:
            url = "https://proxylist.geonode.com/api/proxy-list?limit=500&page=1&country=RU&speed=fast&protocols=http,https,socks4,socks5"
            async with self.session.get(url, timeout=10) as response:
                if response.status == 200:
                    data = await response.json()
                    proxy_list = []
                    for proxy in data.get('data', []):
                        ip = proxy.get('ip')
                        port = proxy.get('port')
                        if ip and port:
                            proxy_list.append(f"{ip}:{port}")
                    self.proxies.extend(proxy_list)
                    self.russian_proxies.extend(proxy_list)
                    console.print(f"–ü–æ–ª—É—á–µ–Ω–æ {len(proxy_list)} —Ä–æ—Å—Å–∏–π—Å–∫–∏—Ö –ø—Ä–æ–∫—Å–∏ –æ—Ç geonode.com")
        except Exception as e:
            console.print(f"[red]–û—à–∏–±–∫–∞ –ø—Ä–∏ –ø–æ–ª—É—á–µ–Ω–∏–∏ –ø—Ä–æ–∫—Å–∏ –æ—Ç geonode.com: {e}")
    
    async def get_proxies_from_free_proxy_list(self):
        try:
            url = "https://free-proxy-list.net/"
            async with self.session.get(url, timeout=10) as response:
                if response.status == 200:
                    text = await response.text()
                    soup = BeautifulSoup(text, 'html.parser')
                    rows = soup.select('#list tbody tr')
                    proxy_list = []
                    for row in rows:
                        columns = row.select('td')
                        if len(columns) >= 8:
                            ip = columns[0].text.strip()
                            port = columns[1].text.strip()
                            country_code = columns[2].text.strip()
                            if country_code == "RU":
                                proxy_list.append(f"{ip}:{port}")
                    self.proxies.extend(proxy_list)
                    self.russian_proxies.extend(proxy_list)
                    console.print(f"–ü–æ–ª—É—á–µ–Ω–æ {len(proxy_list)} —Ä–æ—Å—Å–∏–π—Å–∫–∏—Ö –ø—Ä–æ–∫—Å–∏ –æ—Ç free-proxy-list.net")
        except Exception as e:
            console.print(f"[red]–û—à–∏–±–∫–∞ –ø—Ä–∏ –ø–æ–ª—É—á–µ–Ω–∏–∏ –ø—Ä–æ–∫—Å–∏ –æ—Ç free-proxy-list.net: {e}")

    async def get_proxies_from_proxy_list_download(self):
        try:
            url = "https://www.proxy-list.download/api/v2/get?l=en&t=http&c=Russian+Federation"
            async with self.session.get(url, timeout=10) as response:
                if response.status == 200:
                    text = await response.text()
                    try:
                        data = json.loads(text)
                        proxy_list = []
                        for proxy in data.get('LISTA', []):
                            ip = proxy.get('IP')
                            port = proxy.get('PORT')
                            if ip and port:
                                proxy_list.append(f"{ip}:{port}")
                        self.proxies.extend(proxy_list)
                        self.russian_proxies.extend(proxy_list)
                        console.print(f"–ü–æ–ª—É—á–µ–Ω–æ {len(proxy_list)} —Ä–æ—Å—Å–∏–π—Å–∫–∏—Ö –ø—Ä–æ–∫—Å–∏ –æ—Ç proxy-list.download v2")
                    except json.JSONDecodeError:
                        pass
        except Exception as e:
            console.print(f"[red]–û—à–∏–±–∫–∞ –ø—Ä–∏ –ø–æ–ª—É—á–µ–Ω–∏–∏ –ø—Ä–æ–∫—Å–∏ –æ—Ç proxy-list.download v2: {e}")
    
    async def get_proxies_from_proxy_list_org(self):
        try:
            url = "https://proxy-list.org/russian/index.php"
            async with self.session.get(url, timeout=10) as response:
                if response.status == 200:
                    text = await response.text()
                    soup = BeautifulSoup(text, 'html.parser')
                    proxy_blocks = soup.select('.table ul li')
                    proxy_list = []
                    for block in proxy_blocks:
                        proxy_text = block.text.strip()
                        match = re.search(r'(\d+\.\d+\.\d+\.\d+):(\d+)', proxy_text)
                        if match:
                            ip, port = match.groups()
                            proxy_list.append(f"{ip}:{port}")
                    self.proxies.extend(proxy_list)
                    console.print(f"–ü–æ–ª—É—á–µ–Ω–æ {len(proxy_list)} –ø—Ä–æ–∫—Å–∏ –æ—Ç proxy-list.org")
        except Exception as e:
            console.print(f"[red]–û—à–∏–±–∫–∞ –ø—Ä–∏ –ø–æ–ª—É—á–µ–Ω–∏–∏ –ø—Ä–æ–∫—Å–∏ –æ—Ç proxy-list.org: {e}")

    # –ù–æ–≤—ã–µ —Å–ø–µ—Ü–∏–∞–ª–∏–∑–∏—Ä–æ–≤–∞–Ω–Ω—ã–µ –∏—Å—Ç–æ—á–Ω–∏–∫–∏ —Ä–æ—Å—Å–∏–π—Å–∫–∏—Ö –ø—Ä–æ–∫—Å–∏
    async def get_proxies_from_proxyscrape_ru(self):
        try:
            url = "https://api.proxyscrape.com/v2/?request=displayproxies&protocol=http&timeout=10000&country=RU&ssl=all&anonymity=all"
            async with self.session.get(url, timeout=10) as response:
                if response.status == 200:
                    text = await response.text()
                    proxy_list = text.strip().split('\n')
                    self.proxies.extend(proxy_list)
                    self.russian_proxies.extend(proxy_list)
                    console.print(f"–ü–æ–ª—É—á–µ–Ω–æ {len(proxy_list)} –ø—Ä–æ–∫—Å–∏ –æ—Ç proxyscrape.com (RU)")
        except Exception as e:
            console.print(f"[red]–û—à–∏–±–∫–∞ –ø—Ä–∏ –ø–æ–ª—É—á–µ–Ω–∏–∏ –ø—Ä–æ–∫—Å–∏ –æ—Ç proxyscrape.com: {e}")

    async def get_proxies_from_proxyservers_ru(self):
        try:
            url = "https://ru.proxyservers.pro/proxy/list?country=RU&type=http"
            headers = {
                'User-Agent': 'Mozilla/5.0 (Windows NT 10.0; Win64; x64) AppleWebKit/537.36 (KHTML, like Gecko) Chrome/96.0.4664.110 Safari/537.36'
            }
            async with self.session.get(url, headers=headers, timeout=10) as response:
                if response.status == 200:
                    text = await response.text()
                    soup = BeautifulSoup(text, 'html.parser')
                    rows = soup.select('table.proxy-list tbody tr')
                    proxy_list = []
                    for row in rows:
                        columns = row.select('td')
                        if len(columns) >= 2:
                            ip = columns[1].text.strip()
                            port = columns[2].text.strip()
                            proxy_list.append(f"{ip}:{port}")
                    self.proxies.extend(proxy_list)
                    console.print(f"–ü–æ–ª—É—á–µ–Ω–æ {len(proxy_list)} –ø—Ä–æ–∫—Å–∏ –æ—Ç proxyservers.pro (RU)")
        except Exception as e:
            console.print(f"[red]–û—à–∏–±–∫–∞ –ø—Ä–∏ –ø–æ–ª—É—á–µ–Ω–∏–∏ –ø—Ä–æ–∫—Å–∏ –æ—Ç proxyservers.pro: {e}")

    async def get_proxies_from_2ip_ru(self):
        try:
            url = "https://2ip.ru/proxy/?filter_country=RU&filter_port=&filter_type=any&filter_anon=any"
            headers = {
                'User-Agent': 'Mozilla/5.0 (Windows NT 10.0; Win64; x64) AppleWebKit/537.36 (KHTML, like Gecko) Chrome/96.0.4664.110 Safari/537.36'
            }
            async with self.session.get(url, headers=headers, timeout=10) as response:
                if response.status == 200:
                    text = await response.text()
                    soup = BeautifulSoup(text, 'html.parser')
                    proxy_table = soup.select_one('.proxy__table')
                    if proxy_table:
                        rows = proxy_table.select('tbody tr')
                        proxy_list = []
                        for row in rows:
                            columns = row.select('td')
                            if len(columns) >= 2:
                                ip = columns[0].text.strip()
                                port = columns[1].text.strip()
                                proxy_list.append(f"{ip}:{port}")
                        self.proxies.extend(proxy_list)
                        console.print(f"–ü–æ–ª—É—á–µ–Ω–æ {len(proxy_list)} –ø—Ä–æ–∫—Å–∏ –æ—Ç 2ip.ru (RU)")
        except Exception as e:
            console.print(f"[red]–û—à–∏–±–∫–∞ –ø—Ä–∏ –ø–æ–ª—É—á–µ–Ω–∏–∏ –ø—Ä–æ–∫—Å–∏ –æ—Ç 2ip.ru: {e}")

    async def get_proxies_from_proxy24_net_ru(self):
        try:
            url = "https://proxy24.net/ru-proxy"
            headers = {
                'User-Agent': 'Mozilla/5.0 (Windows NT 10.0; Win64; x64) AppleWebKit/537.36 (KHTML, like Gecko) Chrome/96.0.4664.110 Safari/537.36'
            }
            async with self.session.get(url, headers=headers, timeout=10) as response:
                if response.status == 200:
                    text = await response.text()
                    soup = BeautifulSoup(text, 'html.parser')
                    rows = soup.select('table.table-striped tbody tr')
                    proxy_list = []
                    for row in rows:
                        columns = row.select('td')
                        if len(columns) >= 2:
                            ip = columns[0].text.strip()
                            port = columns[1].text.strip()
                            country = columns[3].text.strip() if len(columns) > 3 else ""
                            if "RU" in country:
                                proxy_list.append(f"{ip}:{port}")
                    self.proxies.extend(proxy_list)
                    console.print(f"–ü–æ–ª—É—á–µ–Ω–æ {len(proxy_list)} –ø—Ä–æ–∫—Å–∏ –æ—Ç proxy24.net (RU)")
        except Exception as e:
            console.print(f"[red]–û—à–∏–±–∫–∞ –ø—Ä–∏ –ø–æ–ª—É—á–µ–Ω–∏–∏ –ø—Ä–æ–∫—Å–∏ –æ—Ç proxy24.net: {e}")

    async def verify_russian_proxies(self):
        """–ü—Ä–æ–≤–µ—Ä–∫–∞, —á—Ç–æ –ø—Ä–æ–∫—Å–∏ –¥–µ–π—Å—Ç–≤–∏—Ç–µ–ª—å–Ω–æ –∏–∑ –†–æ—Å—Å–∏–∏"""
        proxies_to_check = set(self.proxies) - set(self.russian_proxies)
        console.print(f"[yellow]–ü—Ä–æ–≤–µ—Ä–∫–∞ –µ—â–µ {len(proxies_to_check)} –ø—Ä–æ–∫—Å–∏ –Ω–∞ –ø—Ä–∏–Ω–∞–¥–ª–µ–∂–Ω–æ—Å—Ç—å –∫ –†–æ—Å—Å–∏–∏...")
        
        tasks = []
        for proxy in proxies_to_check:  # –ü—Ä–æ–≤–µ—Ä—è–µ–º —Ç–æ–ª—å–∫–æ —Ç–µ –ø—Ä–æ–∫—Å–∏, –∫–æ—Ç–æ—Ä—ã–µ –µ—â–µ –Ω–µ –±—ã–ª–∏ –¥–æ–±–∞–≤–ª–µ–Ω—ã –≤ russian_proxies
            tasks.append(self.check_proxy_country(proxy))
        
        if tasks:  # –ü—Ä–æ–≤–µ—Ä—è–µ–º —Ç–æ–ª—å–∫–æ –µ—Å–ª–∏ –µ—Å—Ç—å –ø—Ä–æ–∫—Å–∏ –¥–ª—è –ø—Ä–æ–≤–µ—Ä–∫–∏
            await asyncio.gather(*tasks)
            
        console.print(f"[bold green]–ù–∞–π–¥–µ–Ω–æ {len(self.russian_proxies)} —Ä–æ—Å—Å–∏–π—Å–∫–∏—Ö –ø—Ä–æ–∫—Å–∏")

    async def check_proxy_country(self, proxy):
        """–ü—Ä–æ–≤–µ—Ä–∫–∞ —Å—Ç—Ä–∞–Ω—ã –ø—Ä–æ–∫—Å–∏ —á–µ—Ä–µ–∑ ipinfo.io –∏ –¥—Ä—É–≥–∏–µ —Å–µ—Ä–≤–∏—Å—ã"""
        ip = proxy.split(':')[0]
        try:
            # –ü—Ä–æ–≤–µ—Ä—è–µ–º —Å—Ç—Ä–∞–Ω—É —á–µ—Ä–µ–∑ ipinfo.io (–±–µ–∑ —Ç–æ–∫–µ–Ω–∞ - –ª–∏–º–∏—Ç 1000 –∑–∞–ø—Ä–æ—Å–æ–≤/–¥–µ–Ω—å)
            url = f"https://ipinfo.io/{ip}/json"
            async with self.session.get(url, timeout=5) as response:
                if response.status == 200:
                    data = await response.json()
                    country = data.get('country')
                    if country == "RU":
                        self.russian_proxies.append(proxy)
                        console.print(f"[green]–ü—Ä–æ–∫—Å–∏ {proxy} –ø–æ–¥—Ç–≤–µ—Ä–∂–¥—ë–Ω –∫–∞–∫ —Ä–æ—Å—Å–∏–π—Å–∫–∏–π")
        except Exception:
            # –ó–∞–ø–∞—Å–Ω–æ–π –≤–∞—Ä–∏–∞–Ω—Ç - –ø—Ä–æ–≤–µ—Ä–∫–∞ —á–µ—Ä–µ–∑ ip-api.com
            try:
                url = f"http://ip-api.com/json/{ip}"
                async with self.session.get(url, timeout=5) as response:
                    if response.status == 200:
                        data = await response.json()
                        country_code = data.get('countryCode')
                        if country_code == "RU":
                            self.russian_proxies.append(proxy)
                            console.print(f"[green]–ü—Ä–æ–∫—Å–∏ {proxy} –ø–æ–¥—Ç–≤–µ—Ä–∂–¥—ë–Ω –∫–∞–∫ —Ä–æ—Å—Å–∏–π—Å–∫–∏–π (—Ä–µ–∑–µ—Ä–≤–Ω—ã–π –º–µ—Ç–æ–¥)")
            except Exception as e:
                pass  # –ò–≥–Ω–æ—Ä–∏—Ä—É–µ–º –æ—à–∏–±–∫–∏

    async def save_proxies(self):
        """
        –°–æ—Ö—Ä–∞–Ω—è–µ–º –Ω–∞–π–¥–µ–Ω–Ω—ã–µ —Ä–æ—Å—Å–∏–π—Å–∫–∏–µ –ø—Ä–æ–∫—Å–∏ –≤ —Ñ–∞–π–ª
        """
        output_file = os.path.join(DATA_DIR, "russian_proxies.txt")
        with open(output_file, "w") as f:
            for proxy in self.russian_proxies:
                f.write(f"{proxy}\n")
        console.print(f"[bold]–°–æ—Ö—Ä–∞–Ω–µ–Ω–æ {len(self.russian_proxies)} —Ä–æ—Å—Å–∏–π—Å–∫–∏—Ö –ø—Ä–æ–∫—Å–∏ –≤ {output_file}")

    async def check_vats_access(self):
        """–ü—Ä–æ–≤–µ—Ä–∫–∞ –¥–æ—Å—Ç—É–ø–∞ –∫ VATS —á–µ—Ä–µ–∑ –Ω–∞–π–¥–µ–Ω–Ω—ã–µ —Ä–æ—Å—Å–∏–π—Å–∫–∏–µ –ø—Ä–æ–∫—Å–∏"""
        console.print("[bold]–ü—Ä–æ–≤–µ—Ä–∫–∞ –¥–æ—Å—Ç—É–ø–∞ –∫ VATS —á–µ—Ä–µ–∑ —Ä–æ—Å—Å–∏–π—Å–∫–∏–µ –ø—Ä–æ–∫—Å–∏...")
        
        working_proxies = []
        vats_url = "http://vats290368.megapbx.ru/"
        
        # –ë—É–¥–µ–º –∏—Å–ø–æ–ª—å–∑–æ–≤–∞—Ç—å —Å–µ–º–∞—Ñ–æ—Ä –¥–ª—è –æ–≥—Ä–∞–Ω–∏—á–µ–Ω–∏—è –∫–æ–ª–∏—á–µ—Å—Ç–≤–∞ –æ–¥–Ω–æ–≤—Ä–µ–º–µ–Ω–Ω—ã—Ö –∑–∞–ø—Ä–æ—Å–æ–≤
        max_concurrent = 20  # –ú–∞–∫—Å–∏–º–∞–ª—å–Ω–æ–µ –∫–æ–ª–∏—á–µ—Å—Ç–≤–æ –æ–¥–Ω–æ–≤—Ä–µ–º–µ–Ω–Ω—ã—Ö –∑–∞–ø—Ä–æ—Å–æ–≤
        semaphore = asyncio.Semaphore(max_concurrent)
        
        # –•–∞—Ä–∞–∫—Ç–µ—Ä–Ω—ã–µ –ø—Ä–∏–∑–Ω–∞–∫–∏ —Ñ–æ—Ä–º—ã –≤—Ö–æ–¥–∞
        login_indicators = [
            'input[name="login"]', 'input[name="username"]', 
            'input[type="password"]', 'form', '<form',
            '–õ–æ–≥–∏–Ω', '–ü–∞—Ä–æ–ª—å', '–í—Ö–æ–¥', '–ê–≤—Ç–æ—Ä–∏–∑–∞—Ü–∏—è',
            '–õ–∏—á–Ω—ã–π –∫–∞–±–∏–Ω–µ—Ç', '–í–∏—Ä—Ç—É–∞–ª—å–Ω–æ–π –ê–¢–°'
        ]
        
        async def check_single_proxy(proxy):
            """–ê—Å–∏–Ω—Ö—Ä–æ–Ω–Ω–∞—è –ø—Ä–æ–≤–µ—Ä–∫–∞ –æ–¥–Ω–æ–≥–æ –ø—Ä–æ–∫—Å–∏"""
            try:
                async with semaphore:
                    # –ò—Å–ø–æ–ª—å–∑—É–µ–º aiohttp –≤–º–µ—Å—Ç–æ requests –¥–ª—è –∞—Å–∏–Ω—Ö—Ä–æ–Ω–Ω—ã—Ö –∑–∞–ø—Ä–æ—Å–æ–≤
                    proxy_url = f"http://{proxy}"
                    timeout = aiohttp.ClientTimeout(total=5)  # –°–Ω–∏–∂–∞–µ–º —Ç–∞–π–º–∞—É—Ç –¥–ª—è —É—Å–∫–æ—Ä–µ–Ω–∏—è –ø—Ä–æ–≤–µ—Ä–∫–∏
                    
                    async with aiohttp.ClientSession(timeout=timeout) as session:
                        try:
                            async with session.get(vats_url, proxy=proxy_url, ssl=False) as response:
                                if response.status == 200:
                                    text = await response.text()
                                    html_content = text.lower()
                                    
                                    # –ü—Ä–æ–≤–µ—Ä—è–µ–º –Ω–∞–ª–∏—á–∏–µ –¥–∏–∞–≥–Ω–æ—Å—Ç–∏—á–µ—Å–∫–∏—Ö –¥–∞–Ω–Ω—ã—Ö (–∏–Ω—Ñ–æ—Ä–º–∞—Ü–∏—è –æ –∑–∞–ø—Ä–æ—Å–µ), –∑–Ω–∞—á–∏—Ç —ç—Ç–æ –Ω–µ –Ω–∞—Å—Ç–æ—è—â–∏–π –∏–Ω—Ç–µ—Ä—Ñ–µ–π—Å
                                    if "remote_addr" in html_content or "request_method" in html_content:
                                        console.print(f"[yellow]‚ö†Ô∏è –ü—Ä–æ–∫—Å–∏ {proxy} –≤–æ–∑–≤—Ä–∞—â–∞–µ—Ç —Ç–æ–ª—å–∫–æ –¥–∏–∞–≥–Ω–æ—Å—Ç–∏—á–µ—Å–∫–∏–µ –¥–∞–Ω–Ω—ã–µ")
                                        return None
                                    
                                    # –ü—Ä–æ–≤–µ—Ä—è–µ–º –Ω–∞–ª–∏—á–∏–µ –ø—Ä–∏–∑–Ω–∞–∫–æ–≤ —Ñ–æ—Ä–º—ã –≤—Ö–æ–¥–∞
                                    for indicator in login_indicators:
                                        if indicator.lower() in html_content:
                                            console.print(f"[bold green]‚úÖ –ü—Ä–æ–∫—Å–∏ {proxy} —É—Å–ø–µ—à–Ω–æ –æ—Ç–∫—Ä—ã–≤–∞–µ—Ç —Ñ–æ—Ä–º—É –≤—Ö–æ–¥–∞ VATS!")
                                            return proxy
                                    
                                    console.print(f"[yellow]‚ö†Ô∏è –ü—Ä–æ–∫—Å–∏ {proxy} –æ—Ç–∫—Ä—ã–≤–∞–µ—Ç —Å—Ç—Ä–∞–Ω–∏—Ü—É, –Ω–æ —Ñ–æ—Ä–º–∞ –≤—Ö–æ–¥–∞ –Ω–µ –Ω–∞–π–¥–µ–Ω–∞")
                                else:
                                    console.print(f"[red]‚ùå –ü—Ä–æ–∫—Å–∏ {proxy} –≤–µ—Ä–Ω—É–ª –∫–æ–¥ {response.status}")
                        except aiohttp.ClientError as e:
                            console.print(f"[red]‚ùå –û—à–∏–±–∫–∞ –ø—Ä–∏ –ø—Ä–æ–≤–µ—Ä–∫–µ {proxy}: {str(e)[:50]}...")
            except Exception as e:
                console.print(f"[red]‚ùå –û—à–∏–±–∫–∞ –ø—Ä–∏ –ø—Ä–æ–≤–µ—Ä–∫–µ {proxy}: {str(e)[:50]}...")
            return None
        
        # –ó–∞–ø—É—Å–∫–∞–µ–º –ø—Ä–æ–≤–µ—Ä–∫—É –≤—Å–µ—Ö –ø—Ä–æ–∫—Å–∏
        console.print(f"[blue]–ü–∞—Ä–∞–ª–ª–µ–ª—å–Ω–∞—è –ø—Ä–æ–≤–µ—Ä–∫–∞ {len(self.russian_proxies)} –ø—Ä–æ–∫—Å–∏ (–º–∞–∫—Å–∏–º–∞–ª—å–Ω–æ {max_concurrent} –æ–¥–Ω–æ–≤—Ä–µ–º–µ–Ω–Ω–æ)...")
        tasks = [check_single_proxy(proxy) for proxy in self.russian_proxies]
        results = await asyncio.gather(*tasks)
        
        # –§–∏–ª—å—Ç—Ä—É–µ–º —Ä–µ–∑—É–ª—å—Ç–∞—Ç—ã, —É–¥–∞–ª—è—è None –∑–Ω–∞—á–µ–Ω–∏—è (–Ω–µ—Ä–∞–±–æ—Ç–∞—é—â–∏–µ –ø—Ä–æ–∫—Å–∏)
        working_proxies = [proxy for proxy in results if proxy is not None]
        
        # –°–æ—Ö—Ä–∞–Ω—è–µ–º —Ä–∞–±–æ—á–∏–µ –ø—Ä–æ–∫—Å–∏ –≤ –æ—Ç–¥–µ–ª—å–Ω—ã–π —Ñ–∞–π–ª
        if working_proxies:
            output_file = os.path.join(DATA_DIR, "vats_working_proxies.txt")
            with open(output_file, "w") as f:
                for proxy in working_proxies:
                    f.write(f"{proxy}\n")
            console.print(f"[bold green]–°–æ—Ö—Ä–∞–Ω–µ–Ω–æ {len(working_proxies)} —Ä–∞–±–æ—á–∏—Ö –ø—Ä–æ–∫—Å–∏ –¥–ª—è VATS –≤ {output_file}")
        else:
            console.print("[bold red]–ù–µ –Ω–∞–π–¥–µ–Ω–æ –Ω–∏ –æ–¥–Ω–æ–≥–æ –ø—Ä–æ–∫—Å–∏, –∫–æ—Ç–æ—Ä—ã–π –º–æ–∂–µ—Ç –æ—Ç–∫—Ä—ã—Ç—å VATS —Å —Ñ–æ—Ä–º–æ–π –≤—Ö–æ–¥–∞")

        return working_proxies

    # –ù–æ–≤—ã–µ –º–µ—Ç–æ–¥—ã –¥–ª—è –ø–æ–ª—É—á–µ–Ω–∏—è –ø—Ä–æ–∫—Å–∏
    async def get_proxies_from_htmlweb_api(self):
        try:
            url = "https://htmlweb.ru/analiz/api_proxy.php?country=ru&format=json"
            async with self.session.get(url, timeout=10) as response:
                if response.status == 200:
                    data = await response.json()
                    proxy_list = []
                    if isinstance(data, dict) and 'list' in data:
                        for proxy_data in data['list']:
                            ip = proxy_data.get('ip')
                            port = proxy_data.get('port')
                            if ip and port:
                                proxy_list.append(f"{ip}:{port}")
                    self.proxies.extend(proxy_list)
                    console.print(f"–ü–æ–ª—É—á–µ–Ω–æ {len(proxy_list)} –ø—Ä–æ–∫—Å–∏ –æ—Ç htmlweb.ru API")
        except Exception as e:
            console.print(f"[red]–û—à–∏–±–∫–∞ –ø—Ä–∏ –ø–æ–ª—É—á–µ–Ω–∏–∏ –ø—Ä–æ–∫—Å–∏ –æ—Ç htmlweb.ru API: {e}")

    async def get_proxies_from_proxy5_net(self):
        try:
            url = "https://proxy5.net/ru/free-proxy/russia"
            headers = {
                'User-Agent': 'Mozilla/5.0 (Windows NT 10.0; Win64; x64) AppleWebKit/537.36 (KHTML, like Gecko) Chrome/96.0.4664.110 Safari/537.36'
            }
            async with self.session.get(url, headers=headers, timeout=10) as response:
                if response.status == 200:
                    text = await response.text()
                    soup = BeautifulSoup(text, 'html.parser')
                    proxy_list = []
                    
                    # –ü–æ–∏—Å–∫ —Ç–∞–±–ª–∏—Ü—ã —Å –ø—Ä–æ–∫—Å–∏
                    proxy_table = soup.select_one('table.proxy-table')
                    if proxy_table:
                        rows = proxy_table.select('tbody tr')
                        for row in rows:
                            columns = row.select('td')
                            if len(columns) >= 2:
                                ip = columns[0].text.strip()
                                port = columns[1].text.strip()
                                proxy_list.append(f"{ip}:{port}")
                    self.proxies.extend(proxy_list)
                    console.print(f"–ü–æ–ª—É—á–µ–Ω–æ {len(proxy_list)} –ø—Ä–æ–∫—Å–∏ –æ—Ç proxy5.net")
        except Exception as e:
            console.print(f"[red]–û—à–∏–±–∫–∞ –ø—Ä–∏ –ø–æ–ª—É—á–µ–Ω–∏–∏ –ø—Ä–æ–∫—Å–∏ –æ—Ç proxy5.net: {e}")

    async def get_proxies_from_fineproxy_org(self):
        try:
            url = "https://fineproxy.org/ru/free-proxies/europe/russia/"
            headers = {
                'User-Agent': 'Mozilla/5.0 (Windows NT 10.0; Win64; x64) AppleWebKit/537.36 (KHTML, like Gecko) Chrome/96.0.4664.110 Safari/537.36'
            }
            async with self.session.get(url, headers=headers, timeout=10) as response:
                if response.status == 200:
                    text = await response.text()
                    soup = BeautifulSoup(text, 'html.parser')
                    proxy_list = []
                    
                    # –ü–æ–∏—Å–∫ —Ç–∞–±–ª–∏—Ü—ã —Å –ø—Ä–æ–∫—Å–∏
                    proxy_table = soup.select_one('table.proxy__list')
                    if proxy_table:
                        rows = proxy_table.select('tbody tr')
                        for row in rows:
                            columns = row.select('td')
                            if len(columns) >= 2:
                                ip = columns[0].text.strip()
                                port = columns[1].text.strip()
                                proxy_list.append(f"{ip}:{port}")
                    self.proxies.extend(proxy_list)
                    console.print(f"–ü–æ–ª—É—á–µ–Ω–æ {len(proxy_list)} –ø—Ä–æ–∫—Å–∏ –æ—Ç fineproxy.org")
        except Exception as e:
            console.print(f"[red]–û—à–∏–±–∫–∞ –ø—Ä–∏ –ø–æ–ª—É—á–µ–Ω–∏–∏ –ø—Ä–æ–∫—Å–∏ –æ—Ç fineproxy.org: {e}")

    async def get_proxies_from_proxyfreeonly(self):
        try:
            url = "https://proxyfreeonly.com/ru/free-proxy-list/russia"
            headers = {
                'User-Agent': 'Mozilla/5.0 (Windows NT 10.0; Win64; x64) AppleWebKit/537.36 (KHTML, like Gecko) Chrome/96.0.4664.110 Safari/537.36'
            }
            async with self.session.get(url, headers=headers, timeout=10) as response:
                if response.status == 200:
                    text = await response.text()
                    soup = BeautifulSoup(text, 'html.parser')
                    proxy_list = []
                    
                    # –ü–æ–∏—Å–∫ —Ç–∞–±–ª–∏—Ü—ã —Å –ø—Ä–æ–∫—Å–∏
                    proxy_table = soup.select_one('table.proxy-table')
                    if proxy_table:
                        rows = proxy_table.select('tbody tr')
                        for row in rows:
                            columns = row.select('td')
                            if len(columns) >= 2:
                                ip = columns[0].text.strip()
                                port = columns[1].text.strip()
                                proxy_list.append(f"{ip}:{port}")
                    self.proxies.extend(proxy_list)
                    console.print(f"–ü–æ–ª—É—á–µ–Ω–æ {len(proxy_list)} –ø—Ä–æ–∫—Å–∏ –æ—Ç proxyfreeonly.com")
        except Exception as e:
            console.print(f"[red]–û—à–∏–±–∫–∞ –ø—Ä–∏ –ø–æ–ª—É—á–µ–Ω–∏–∏ –ø—Ä–æ–∫—Å–∏ –æ—Ç proxyfreeonly.com: {e}")

    async def get_proxies_from_good_proxies_ru(self):
        try:
            url = "https://www.good-proxies.ru/free-proxy"
            headers = {
                'User-Agent': 'Mozilla/5.0 (Windows NT 10.0; Win64; x64) AppleWebKit/537.36 (KHTML, like Gecko) Chrome/96.0.4664.110 Safari/537.36'
            }
            async with self.session.get(url, headers=headers, timeout=10) as response:
                if response.status == 200:
                    text = await response.text()
                    soup = BeautifulSoup(text, 'html.parser')
                    proxy_list = []
                    
                    # –ü–æ–∏—Å–∫ —Ç–∞–±–ª–∏—Ü—ã —Å –ø—Ä–æ–∫—Å–∏
                    proxy_table = soup.select('table.proxy-list')
                    for table in proxy_table:
                        rows = table.select('tbody tr')
                        for row in rows:
                            columns = row.select('td')
                            if len(columns) >= 2:
                                ip = columns[0].text.strip()
                                port = columns[1].text.strip()
                                country = columns[2].text.strip() if len(columns) > 2 else ""
                                if "RU" in country or "–†–æ—Å—Å–∏—è" in country:
                                    proxy_list.append(f"{ip}:{port}")
                    self.proxies.extend(proxy_list)
                    console.print(f"–ü–æ–ª—É—á–µ–Ω–æ {len(proxy_list)} –ø—Ä–æ–∫—Å–∏ –æ—Ç good-proxies.ru")
        except Exception as e:
            console.print(f"[red]–û—à–∏–±–∫–∞ –ø—Ä–∏ –ø–æ–ª—É—á–µ–Ω–∏–∏ –ø—Ä–æ–∫—Å–∏ –æ—Ç good-proxies.ru: {e}")

    async def get_proxies_from_iproyal_ru(self):
        try:
            url = "https://iproyal.com/ru/free-proxies/russia-ru/"
            headers = {
                'User-Agent': 'Mozilla/5.0 (Windows NT 10.0; Win64; x64) AppleWebKit/537.36 (KHTML, like Gecko) Chrome/96.0.4664.110 Safari/537.36'
            }
            async with self.session.get(url, headers=headers, timeout=10) as response:
                if response.status == 200:
                    text = await response.text()
                    soup = BeautifulSoup(text, 'html.parser')
                    proxy_list = []
                    
                    # –ü–æ–∏—Å–∫ —Ç–∞–±–ª–∏—Ü—ã —Å –ø—Ä–æ–∫—Å–∏
                    proxy_table = soup.select_one('table.proxies-table')
                    if proxy_table:
                        rows = proxy_table.select('tbody tr')
                        for row in rows:
                            columns = row.select('td')
                            if len(columns) >= 2:
                                ip = columns[0].text.strip()
                                port = columns[1].text.strip()
                                proxy_list.append(f"{ip}:{port}")
                    self.proxies.extend(proxy_list)
                    console.print(f"–ü–æ–ª—É—á–µ–Ω–æ {len(proxy_list)} –ø—Ä–æ–∫—Å–∏ –æ—Ç iproyal.com")
        except Exception as e:
            console.print(f"[red]–û—à–∏–±–∫–∞ –ø—Ä–∏ –ø–æ–ª—É—á–µ–Ω–∏–∏ –ø—Ä–æ–∫—Å–∏ –æ—Ç iproyal.com: {e}")

async def main():
    finder = RussianProxyFinder()
    try:
        await finder.initialize()
        console.print("[bold]–ü–æ–∏—Å–∫ —Ä–æ—Å—Å–∏–π—Å–∫–∏—Ö –ø—Ä–æ–∫—Å–∏...")
        await finder.get_proxies_from_api()
        await finder.verify_russian_proxies()
        await finder.save_proxies()
        
        if finder.russian_proxies:
            working_proxies = await finder.check_vats_access()
            if working_proxies:
                console.print("\n[bold]–†–∞–±–æ—á–∏–µ –ø—Ä–æ–∫—Å–∏ –¥–ª—è –¥–æ—Å—Ç—É–ø–∞ –∫ VATS:")
                table = Table(show_header=True, header_style="bold")
                table.add_column("‚Ññ", style="dim")
                table.add_column("–ü—Ä–æ–∫—Å–∏")
                
                for i, proxy in enumerate(working_proxies, 1):
                    table.add_row(str(i), proxy)
                
                console.print(table)
                console.print("\n[bold]–ò—Å–ø–æ–ª—å–∑—É–π—Ç–µ —ç—Ç–∏ –ø—Ä–æ–∫—Å–∏ –¥–ª—è –¥–æ—Å—Ç—É–ø–∞ –∫ VATS —á–µ—Ä–µ–∑ –±—Ä–∞—É–∑–µ—Ä.")
            else:
                console.print("\n[bold yellow]–†–æ—Å—Å–∏–π—Å–∫–∏–µ –ø—Ä–æ–∫—Å–∏ –Ω–∞–π–¥–µ–Ω—ã, –Ω–æ –Ω–∏ –æ–¥–∏–Ω –Ω–µ –º–æ–∂–µ—Ç –æ—Ç–∫—Ä—ã—Ç—å VATS —Å –ø–æ–ª–Ω—ã–º –∏–Ω—Ç–µ—Ä—Ñ–µ–π—Å–æ–º")
        else:
            console.print("[bold red]–ù–µ –Ω–∞–π–¥–µ–Ω–æ –Ω–∏ –æ–¥–Ω–æ–≥–æ —Ä–æ—Å—Å–∏–π—Å–∫–æ–≥–æ –ø—Ä–æ–∫—Å–∏")
            
    except Exception as e:
        console.print(f"[bold red]–û—à–∏–±–∫–∞: {e}")
    finally:
        await finder.close()

# –ó–∞–ø—É—Å–∫ –ø—Ä–æ–≥—Ä–∞–º–º—ã
if __name__ == "__main__":
    print("\nüîç –ü–æ–∏—Å–∫ —Ä–æ—Å—Å–∏–π—Å–∫–∏—Ö –ø—Ä–æ–∫—Å–∏ –¥–ª—è –¥–æ—Å—Ç—É–ø–∞ –∫ VATS...\n")
    loop = asyncio.get_event_loop()
    loop.run_until_complete(main())